[
  {
    "job_id": "4204054198",
    "title": "Data Engineer (Pyspark)",
    "company": "Virtusa",
    "location": "Bangalore Urban, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/data-engineer-pyspark-at-virtusa-4204054198?position=1&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=GEz9JG%2FTkpV0Iwc3uj319w%3D%3D",
    "description": "About The Role\n\nWe are seeking a highly skilled Data Engineer with deep expertise in PySpark and the Cloudera Data Platform (CDP) to join our data engineering team. As a Data Engineer, you will be responsible for designing, developing, and maintaining scalable data pipelines that ensure high data quality and availability across the organization. This role requires a strong background in big data ecosystems, cloud-native tools, and advanced data processing techniques.\n\nThe ideal candidate has hands-on experience with data ingestion, transformation, and optimization on the Cloudera Data Platform, along with a proven track record of implementing data engineering best practices. You will work closely with other data engineers to build solutions that drive impactful business insights.\n\nResponsibilities\n\nData Pipeline Development: Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\n\nData Ingestion: Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\n\nData Transformation and Processing: Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\n\nPerformance Optimization: Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\n\nData Quality and Validation: Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\n\nAutomation and Orchestration: Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\n\nEducation and Experience\n\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\n\n3+ years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\n\nTechnical Skills\n\nPySpark: Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\n\nCloudera Data Platform: Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\n\nData Warehousing: Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\n\nBig Data Technologies: Familiarity with Hadoop, Kafka, and other distributed computing tools.\n\nOrchestration and Scheduling: Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\n\nScripting and Automation: Strong scripting skills in Linux.\nShow more "
  },
  {
    "job_id": "4143201595",
    "title": "Software Engineer (5-8yrs)",
    "company": "PhonePe",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/software-engineer-5-8yrs-at-phonepe-4143201595?position=2&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=xJOpgR77u%2FpCW7yUuMyUnQ%3D%3D",
    "description": "About PhonePe Limited:\n\nHeadquartered in India, its flagship product, the PhonePe digital payments app, was launched in Aug 2016. As of April 2025, PhonePe has over 60 Crore (600 Million) registered users and a digital payments acceptance network spread across over 4 Crore (40+ million) merchants. PhonePe also processes over 33 Crore (330+ Million) transactions daily with an Annualized Total Payment Value (TPV) of over INR 150 lakh crore.\n\nPhonePe\u2019s portfolio of businesses includes the distribution of financial products (Insurance, Lending, and Wealth) as well as new consumer tech businesses (Pincode - hyperlocal e-commerce and Indus AppStore Localized App Store for the Android ecosystem) in India, which are aligned with the company\u2019s vision to offer every Indian an equal opportunity to accelerate their progress by unlocking the flow of money and access to services.\n\nCulture:\n\nAt PhonePe, we go the extra mile to make sure you can bring your best self to work, Everyday!. And that starts with creating the right environment for you. We empower people and trust them to do the right thing. Here, you own your work from start to finish, right from day one. PhonePe-rs solve complex problems and execute quickly; often building frameworks from scratch. If you\u2019re excited by the idea of building platforms that touch millions, ideating with some of the best minds in the country and executing on your dreams with purpose and speed, join us!\n\nChallenges\n\nBuilding for Scale, Rapid Iterative Development, and Customer-centric Product Thinking at each step defines every day for a developer at PhonePe. Though we engineer for a 50million+ strong user base, we code with every individual user in mind. While we are quick to adopt the latest in Engineering, we care utmost for security, stability, and automation. Apply if you want to experience the best combination of passionate application development and product-driven thinking\n\nAs a Software Engineer:\n\n\nYou will build Robust and scalable web-based applications You will need to think of platforms & reuse\nBuild abstractions and contracts with separation of concerns for a larger scope\nDrive problem-solving skills for high-level business and technical problems.\nDo high-level design with guidance; Functional modeling, break-down of a module\nDo incremental changes to architecture: impact analysis of the same\nDo performance tuning and improvements in large scale distributed systems\nMentor young minds and foster team spirit, break down execution into phases to bring predictability to overall execution\nWork closely with Product Manager to derive capability view from features/solutions, Lead execution of medium-sized projects\nWork with broader stakeholders to track the impact of projects/features and proactively iterate to improve them As a senior software engineer you must have\nExtensive and expert programming experience in at least one general programming language (e.g. Java, C, C++) & tech stack to write maintainable, scalable, unit-tested code.\nExperience with multi-threading and concurrency programming\nExtensive experience in object-oriented design skills, knowledge of design patterns, and huge passion and ability to design intuitive module and class-level interfaces\nExcellent coding skills \u2013 should be able to convert the design into code fluently\nKnowledge of Test Driven Development\nGood understanding of databases (e.g. MySQL) and NoSQL (e.g. HBase, Elasticsearch, Aerospike, etc)\nStrong desire to solving complex and interesting real-world problems\nExperience with full life cycle development in any programming language on a Linux platform\nGo-getter attitude that reflects in energy and intent behind assigned tasks\nWorked in a startups environment with high levels of ownership and commitment\nBTech, MTech, or Ph.D. in Computer Science or related technical discipline (or equivalent).\nExperience in building highly scalable business applications, which involve implementing large complex business flows and dealing with a huge amount of data.\n9+ years of experience in the art of writing code and solving problems on a Large Scale.\nAn open communicator who shares thoughts and opinions frequently listens intently and takes constructive feedback.\n\n\nAs a Software Engineer, good to have\n\n\nThe ability to drive the design and architecture of multiple subsystems\nAbility to break-down larger/fuzzier problems into smaller ones in the scope of the product\nUnderstanding of the industry\u2019s coding standards and an ability to create appropriate technical documentation.\n\n\nPhonePe Full Time Employee Benefits (Not applicable for Intern or Contract Roles)\n\n\nInsurance Benefits - Medical Insurance, Critical Illness Insurance, Accidental Insurance, Life Insurance\nWellness Program - Employee Assistance Program, Onsite Medical Center, Emergency Support System\nParental Support - Maternity Benefit, Paternity Benefit Program, Adoption Assistance Program, Day-care Support Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Higher Education Assistance, Car Lease, Salary Advance Policy\n\n\nOur inclusive culture promotes individual expression, creativity, innovation, and achievement and in turn helps us better understand and serve our customers. We see ourselves as a place for intellectual curiosity, ideas and debates, where diverse perspectives lead to deeper understanding and better quality results. PhonePe is an equal opportunity employer and is committed to treating all its employees and job applicants equally; regardless of gender, sexual preference, religion, race, color or disability. If you have a disability or special need that requires assistance or reasonable accommodation, during the application and hiring process, including support for the interview or onboarding process, please fill out this form.\n\nRead more about PhonePe on our blog.\n\nLife at PhonePe\n\nPhonePe in the news\nShow more "
  },
  {
    "job_id": "4272740793",
    "title": "Associate Staff Engineer (Data Science)",
    "company": "Nagarro",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/associate-staff-engineer-data-science-at-nagarro-4272740793?position=4&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=7Umtdso3UOV2qKrCHo%2BY%2FA%3D%3D",
    "description": "Company Description\n\n\ud83d\udc4b\ud83c\udffcWe're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at a scale \u2014 across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 36 countries). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nJob Description\n\nREQUIREMENTS:\n\n\nMust have experience in Data Science Fundamentals, Data Analysis & Visualization, Statistics & Probability, SQL, Model Packaging & Deployment.\nMust have experience in prompt engineering, RAG & Generative AI.\nShould have experience in Python or R for Data Science.\nShould have experience in Classification-regression, Forecasting, Unsupervised Models, Recommendation Systems, Natural Language processing & Optimization.\nShould have experience in Data science on AWS/Azure/GCP.\nStrong troubleshooting skills in different disparate technologies and environments.\nShould be enthusiastic about different areas of work and exploring new technologies.\nShould have clarity of thought and strong communication skills to effectively pitch solutions.\nMust have ability to explore and grasp new technologies.\nMust be capable of mentoring team members in projects and helping them keep up with new technologies.\nShould be empowering the team members to be solution providers and enable a flat environment where everyone\u2019s point of view is considered, and feedback is encouraged.\n\n\nRESPONSIBILITIES:\n\n\nWriting and reviewing great quality code\nUnderstanding the client\u2019s business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements.\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the client\u2019s requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it.\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\nShould be open to travel onsite once a month.\n\n\nQualifications\n\nBachelor\u2019s or master\u2019s degree in computer science, Information Technology, or a related field.\nShow more "
  },
  {
    "job_id": "4270322242",
    "title": "Data Analytics and Engineering- Associate 2",
    "company": "PwC Acceleration Center India",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-25",
    "link": "https://in.linkedin.com/jobs/view/data-analytics-and-engineering-associate-2-at-pwc-acceleration-center-india-4270322242?position=5&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=JL249OL97ZkyB1Iw2fILgw%3D%3D",
    "description": "At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nDriven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\n\nApply a learning mindset and take ownership for your own development.\nAppreciate diverse perspectives, needs, and feelings of others.\nAdopt habits to sustain high performance and develop your potential.\nActively listen, ask questions to check understanding, and clearly express ideas.\nSeek, reflect, act on, and give feedback.\nGather information from a range of sources to analyse facts and discern patterns.\nCommit to understanding how the business works and building commercial awareness.\nLearn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements.\n\n\nBelow are examples of role/skills profiles used by the UK firm when hiring Data Analytics based roles indicated above.\n\nJob Description & Summary\n\nOperate is the firm's delivery engine, serving as the orchestrator of services across the organisation. It is a global team of delivery professionals united by a commitment to excellence and impact. Operate has built a strong reputation for collaboration, mobilising quickly, and effectively getting tasks done. It aims to build a world-class delivery capability, focusing on evolving operational delivery, embedding automation and AI, and raising the bar for quality and consistency. The goal is to add strategic value for clients and contribute to the firm\u2019s ambition of pre-eminence in the market. Team members in Operate are provided with meaningful opportunities to lead, learn, and grow, embracing a future-ready workforce trained in cutting-edge technology. Operate ensures clients can access a single front door to global delivery chains, providing tailored, high-quality solutions to meet evolving challenges.\n\nThe role will be based in Kolkata. However, with a diverse range of clients and projects, you'll occasionally have the exciting opportunity to work in various locations, offering exposure to different industries and cultures. This flexibility opens doors to unique networking experiences and accelerated career growth, enriching your professional journey. Your willingness and ability to do this will be discussed as part of the recruitment process. Candidates who prefer not to travel will still be considered.\n\nRole Description\n\nAs an integral part of our data team, Associate 2 professionals contribute significantly to the development of data management and analytics functions, including our growing Data Services. In this role, you'll assist engagement teams in delivering meaningful insights by helping design, integrate, and analyse data systems. You will work with the latest technologies, especially within the Microsoft ecosystem, to enhance our operational capabilities. Working on a variety of projects, you'll have the chance to contribute your ideas and support innovative solutions. This experience offers opportunities for professional growth and helps cultivate a forward-thinking mindset. As you support our Data Services, you'll gain exposure to the evolving field of data analytics, providing an excellent foundation for building expertise and expanding your career journey.\n\nKey Activities Include, But Are Not Limited To\n\n\nAssisting in the development of data models and frameworks to enhance data governance and efficiency.\nSupporting efforts to address data integration, quality, and management process challenges.\nParticipating in the implementation of best practices in automation to streamline data workflows.\nCollaborating with stakeholders to gather, interpret, and translate data requirements into practical insights and solutions.\nSupport management of data projects alongside senior team members.\nAssist in engaging with clients to understand their data needs.\nWork effectively as part of a team to achieve project goals.\n\n\nEssential Requirements\n\n\nAt least two years of experience in data analytics, with a focus on handling large datasets and supporting the creation of detailed reports.\nFamiliarity with Python and experience in working within a Microsoft Azure environment.\nExposure to data warehousing and data modelling techniques (e.g., dimensional modelling).\nBasic proficiency in PySpark and Databricks/Snowflake/MS Fabric, with foundational SQL skills.\nExperience with orchestration tools like Azure Data Factory (ADF), Airflow, or DBT.\nAwareness of DevOps practices, including introducing CI/CD and release pipelines.\nFamiliarity with Azure DevOps tools and GitHub.\nBasic understanding of Azure SQL DB or other RDBMS systems.\nIntroductory knowledge of GenAI concepts.\n\n\nAdditional Skills / Experiences That Will Be Beneficial\n\n\nUnderstanding of data governance frameworks.\nAwareness of Power Automate functionalities.\n\n\nWHY JOIN US?\n\nThis role is not just about the technical expertise\u2014it\u2019s about being part of something transformational. You'll be part of a vibrant team where growth opportunities are vast and where your contributions directly impact our mission to break new ground in data services. With a work culture that values innovation, collaboration, and personal growth, joining PwC's Operate Data Analytics team offers you the chance to shape the future of operational and data service solutions with creativity and foresight. Dive into exciting projects, challenge the status quo, and drive the narrative forward!\nShow more "
  },
  {
    "job_id": "4272458866",
    "title": "CB - Data Engineer -- L1",
    "company": "CoffeeBeans",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/cb-data-engineer-l1-at-coffeebeans-4272458866?position=6&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=WwAbBa17iWXD%2FwY%2BucjzPA%3D%3D",
    "description": "Experience: 1-3 year\n\nLocation: Bangalore/Pune/Hyderabad\n\nWork mode: Hybrid\n\nFounded in the year 2017, CoffeeBeans specializes in offering high end consulting services in technology, product, and processes. We help our clients attain significant improvement in quality of delivery through impactful product launches, process simplification, and help build competencies that drive business outcomes across industries. The company uses new-age technologies to help its clients build superior products and realize better customer value. We also offer data-driven solutions and AI-based products for businesses operating in a wide range of product categories and service domains\n\nAs a Data Engineer, you will play a crucial role in designing and optimizing data solutions for our clients. The ideal candidate will have a strong foundation in Python, experience with Databricks Warehouse SQL or a similar Spark-based SQL platform, and a deep understanding of performance optimization techniques in the data engineering landscape. Knowledge of serverless approaches, Spark Streaming, Structured Streaming, Delta Live Tables, and related technologies is essential.\n\nWhat are we looking for?\n\n\nBachelor's degree in Computer Science, Engineering, or a related field.\n1-3 year of experience as a Data Engineer.\nProven track record of designing and optimizing data solutions.\nStrong problem-solving and analytical skills.\n\n\n\nMust haves\n\n\nPython: Proficiency in Python for data engineering tasks and scripting.\nPerformance Optimization: Deep understanding and practical experience in optimizing data engineering performance.\nServerless Approaches: Familiarity with serverless approaches in data engineering solutions.\n\n\n\nGood to have\n\n\nDatabricks Warehouse SQL or Equivalent Spark SQL Platform: Hands-on experience with Databricks Warehouse SQL or a similar Spark-based SQL platform.\nSpark Streaming: Experience with Spark Streaming for real-time data processing.\nStructured Streaming: Familiarity with Structured Streaming in Apache Spark.\nDelta Live Tables: Knowledge and practical experience with Delta Live Tables or similar technologies.\n\n\n\nWhat will you be doing?\n\nDesign, develop, and maintain scalable and efficient data solutions.\n\nCollaborate with clients to understand data requirements and provide tailored solutions.\n\nImplement performance optimization techniques in the data engineering landscape.\n\nWork with serverless approaches to enhance scalability and flexibility.\n\nUtilize Spark Streaming and Structured Streaming for real-time data processing.\n\nImplement and manage Delta Live Tables for efficient change data capture.\nShow more "
  },
  {
    "job_id": "4272463283",
    "title": "CB - Data Engineer --L2",
    "company": "CoffeeBeans",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/cb-data-engineer-l2-at-coffeebeans-4272463283?position=7&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=sCutM45x8IpZzDuSiwtekQ%3D%3D",
    "description": "Experience: 3-6 year\n\nLocation: Bangalore/Pune/Hyderabad\n\nWork mode: Hybrid\n\nFounded in the year 2017, CoffeeBeans specializes in offering high end consulting services in technology, product, and processes. We help our clients attain significant improvement in quality of delivery through impactful product launches, process simplification, and help build competencies that drive business outcomes across industries. The company uses new-age technologies to help its clients build superior products and realize better customer value. We also offer data-driven solutions and AI-based products for businesses operating in a wide range of product categories and service domains\n\nAs a Data Engineer, you will play a crucial role in designing and optimizing data solutions for our clients. The ideal candidate will have a strong foundation in Python, experience with Databricks Warehouse SQL or a similar Spark-based SQL platform, and a deep understanding of performance optimization techniques in the data engineering landscape. Knowledge of serverless approaches, Spark Streaming, Structured Streaming, Delta Live Tables, and related technologies is essential.\n\nWhat are we looking for?\n\n\nBachelor's degree in Computer Science, Engineering, or a related field.\n3-6 year of experience as a Data Engineer.\nProven track record of designing and optimizing data solutions.\nStrong problem-solving and analytical skills.\n\n\n\nMust haves\n\n\nPython: Proficiency in Python for data engineering tasks and scripting.\nPerformance Optimization: Deep understanding and practical experience in optimizing data engineering performance.\nServerless Approaches: Familiarity with serverless approaches in data engineering solutions.\n\n\n\nGood to have\n\n\nDatabricks Warehouse SQL or Equivalent Spark SQL Platform: Hands-on experience with Databricks Warehouse SQL or a similar Spark-based SQL platform.\nSpark Streaming: Experience with Spark Streaming for real-time data processing.\nStructured Streaming: Familiarity with Structured Streaming in Apache Spark.\nDelta Live Tables: Knowledge and practical experience with Delta Live Tables or similar technologies.\n\n\n\nWhat will you be doing?\n\nDesign, develop, and maintain scalable and efficient data solutions.\n\nCollaborate with clients to understand data requirements and provide tailored solutions.\n\nImplement performance optimization techniques in the data engineering landscape.\n\nWork with serverless approaches to enhance scalability and flexibility.\n\nUtilize Spark Streaming and Structured Streaming for real-time data processing.\n\nImplement and manage Delta Live Tables for efficient change data capture.\nShow more "
  },
  {
    "job_id": "4234488284",
    "title": "Senior Engineer Consultant-Data Engineering",
    "company": "Verizon",
    "location": "Bengaluru East, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/senior-engineer-consultant-data-engineering-at-verizon-4234488284?position=8&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=HRffBEJ6ULFoUfLD1tSSvw%3D%3D",
    "description": "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely \u2014 even if they\u2019re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love \u2014 driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together \u2014 lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you\u2019ll be doing...\n\nWe\u2019re seeking a skilled Lead Senior Data Engineering Analyst to join our high-performing team and propel our telecom business forward. You\u2019ll contribute to building cutting-edge data products and assets for our wireless and wireline operations, spanning areas like consumer analytics, network performance, and service assurance.\n\nIn this role, you will develop deep expertise in various telecom domains. As part of the Data Architecture & Strategy team, you\u2019ll collaborate closely with IT and business stakeholders to design and implement user-friendly, robust data product solutions. This includes defining data quality and incorporating data classification and governance principles.\n\nYour responsibilities encompass\n\n\nCollaborating with stakeholders to understand data requirements and translate them into efficient data models\nDefining the scope and purpose of data product solutions, collaborating with stakeholders to finalize project blueprints, and overseeing the design process through all phases of the release lifecycle.\nDesigning, developing, and implementing data architecture solutions on GCP and Teradata to support our Telecom business.\nDesigning data ingestion for both real-time and batch processing, ensuring efficient and scalable data acquisition for creating an effective data warehouse.\nFormulating End to End data solutions (Authoritative Data Source, Data Protection, Taxonomy Alignment)\nMaintaining meticulous documentation, including data design specifications, functional test cases, data lineage, and other relevant artifacts for all data product solution assets.\nDefining Data Architecture Strategy (Enterprise & Domain level) and Enterprise Data Model Standards & Ownership\nProactively identifying opportunities for automation and performance optimization within your scope of work\nCollaborating effectively within a product-oriented organization, providing data expertise and solutions across multiple business units.\nCultivating strong cross-functional relationships and establish yourself as a subject matter expert in data and analytics within the organization.\nActing as a mentor to junior team members\n\n\nWhat we\u2019re looking for...\n\nYou\u2019re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.\n\nYou'll need to have\u2026\n\n\nBachelor\u2019s degree or four or more years of work experience.\nFour or more years of relevant work experience.\nFour or more years of relevant work experience in data architecture, data warehousing, or a related role.\nStrong grasp of data architecture principles, best practices, and methodologies.\nExpertise in SQL for data analysis, data discovery, data profiling and solution design.\nExperience defining data standards, data quality and implementing industry best practices for scalable and maintainable data models using data modeling tools like Erwin\nProven experience with ETL, data warehousing concepts, and the data management lifecycle\nSkilled in creating technical documentation, including source-to-target mappings and SLAs.\nExperience in shell scripting and python programming language\nUnderstanding of git version control and basic git command\nHands-on experience with cloud services relevant to data engineering and architecture (e.g., BigQuery, Dataflow, Dataproc, Cloud Storage).\n\n\nEven better if you have one or more of the following\u2026\n\n\nMaster's degree in Computer Science.\nExperience in the Telecommunications industry, with knowledge of wireless and wireline business domains.\nExperience with stream-processing systems, API, Events etc.\nCertification in GCP-Data Engineer/Architect.\nAccuracy and attention to detail.\nGood problem solving, analytical, and research capabilities.\nGood verbal and written communication.\nExperience presenting to and influence stakeholders.\nExperience with large clusters, databases, BI tools, data quality and performance tuning.\nExperience in driving one or more smaller teams for technical delivery\n\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don\u2019t meet every \u201ceven better\u201d qualification listed above.\n\n#AI&D\n\nWhere you\u2019ll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.\nShow more "
  },
  {
    "job_id": "4196665846",
    "title": "Business Intelligence Analyst / Data Engineer",
    "company": "Valtech",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/business-intelligence-analyst-data-engineer-at-valtech-4196665846?position=9&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=JMyEhjZCt98aJXPgNwmJeQ%3D%3D",
    "description": "At Valtech, We\u2019ve Got Opportunities To Offer You \u2014 For Growth, For Leadership, For Big, World-changing Impact And For, Dare We Say It, Fun. We Are a Global Workforce That Is Committed To Building a World That Works Better For Everyone. And That Starts With Our Kin. That\u2019s Why We\u2019re Proud Of\n\nThe role\n\nIdeal candidates will have strong quantitative backgrounds and analytical discipline. While they will have some demonstrated ability to write code, they will not have learned programming languages for the sake of building their resume, but rather as a means to express their intellectual curiosity and analytical voice. Valtech will provide a platform and training to help them reach their full potential.\n\nValtech is looking to hire a Business Intelligence Analyst / Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and look to continually learn and enjoy sharing expertise, read on!\n\nRole Responsibilities\n\n\nAnalyze a collection of raw data sets to create meaningful impact to large enterprise clients while maintaining a high degree of scientific rigor and discipline.\nEngineer data pipelines and products to help stakeholders make and execute data driven decisions.\nCommunicate analytical findings in an intuitive and visually compelling way.\nCreating highly visual and interactive dashboards via Tableau, PowerBI, or custom web applications\nConducting deep dive analysis and designing KPIs to help guide business decisions and measure success\nEngineering data infrastructure, software libraries, and APIs supporting BI and ML data pipelines\nArchitecting cloud data platform components enabling the above\nBuilding and tracking project timelines, dependences, and risks\nGathering stakeholder requirements and conducting technical due diligence toward designing pragmatic data-driven business solutions\n\n\nMinimum Qualifications\n\nWe want all new hires to succeed in their roles at Valtech. That's why we've outlined the job requirements below. To be considered for this role, it's important that you meet all Minimum Qualifications. If you do not meet all of the Preferred Qualifications, we still encourage you to apply.\n\n\nProven industry experience executing data engineering, analytics, and/or data science projects or Bachelors/Masters degree in quantitative studies including Engineering, Mathematics, Statistics, Computer Science or computation-intensive Sciences and Humanities.\nProficiency (can execute data ingestion to insight) in programmatic languages such as SQL, Python, and R.\n\n\nPreferred Qualifications\n\n\nProficiency in visualization/reporting tools such as Tableau and PowerBI or programmatic visualization library such as R ggplot2, Python matplotlib/seaborn/bokeh, Javascript D3.\nProficiency in big data environments and tools such as Spark, Hive, Impala, Pig, etc.\nProficiency with cloud architecture components (AWS, Azure, Google)\nProficiency with data pipeline software such as Airflow, Luigi, or Prefect\nAbility to turn raw data and ambiguous business questions into distilled findings and recommendations for action\nExperience with statistical and machine learning libraries along with the ability to apply them appropriately to business problems\nExperience leading and managing technical data/analytics/machine learning projects\n\n\nWhat We Offer\n\nYou\u2019ll join an international network of data professionals within our organisation. We support continuous development through our dedicated Academy. If you're looking to push the boundaries of innovation and creativity in a culture that values freedom and responsibility, we encourage you to apply.\n\nAt Valtech, we\u2019re here to engineer experiences that work and reach every single person. To do this, we are proactive about creating workplaces that work for every person at Valtech. Our goal is to create an equitable workplace which gives people from all backgrounds the support they need to thrive, grow and meet their goals (whatever they may be). You can find out more about what we\u2019re doing to create a Valtech for everyone here.\n\nPlease do not worry if you do not meet all of the criteria or if you have some gaps in your CV. We\u2019d love to hear from you and see if you\u2019re our next member of the Valtech team!\nShow more "
  },
  {
    "job_id": "4204943125",
    "title": "Data Engineering - Data Design & Curation-Associate-Software Engineering",
    "company": "Goldman Sachs",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/data-engineering-data-design-curation-associate-software-engineering-at-goldman-sachs-4204943125?position=10&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=yIwTjazBKfd8y%2F%2Fo91nfyQ%3D%3D",
    "description": "Job Description\n\nData plays a critical role in every facet of the Goldman Sachs business. The Data Engineering group is at the core of that offering, focusing on providing the platform, processes, and governance, for enabling the availability of clean, organized, and impactful data to scale, streamline, and empower our core businesses.\n\nWithin Data Engineering, we focus on offering a comprehensive data platform, Legend, which we have made available as an open-source product. Legend includes a full data modeling environment, as well as the execution of data access and controls, and a vast set of value-add products which allow our business users to operate more efficiently.\n\nLeveraging our own Legend offering, our engineers build efficient data solutions that source, curate, and distribute critical data to our businesses, including financial product, pricing, transaction, and client reference data. Our engineers collaborate closely with the business to design and curate business-specific data models, and to transform and distribute data for optimal storage and retrieval.\n\nWho We Look For\n\nGoldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.\n\nAs a Full-stack Software Engineer on the Data Engineering team, you will be responsible for helping improve the Legend data platform, our curated data offerings, and how the business uses data. We tackle some of the most complex engineering problems across distributed software development, optimizing data access and delivery, enabling core access controls via well-defined security paradigms, building UIs to enable data visualization, using machine learning to curate data, or engaging with businesses to ensure their data needs are met, and we react quickly to new demands by rapidly evolving our data solutions.\n\nHow You Will Fulfill York Potential\n\n\nDesign & develop modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies\nContribute to an open-source technology - https://github.com/finos/legend\nDrive adoption of cloud technology for data processing and warehousing\nEngage with data consumers and producers in order to design appropriate models to suit enable the business\n\n\nRelevant Technologies: Java, Python, AWS, React\n\nBasic Qualifications\n\n\nA Bachelor or Master degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)\n2-7+ years of relevant work experience in a team-focused environment\n2-7+ years of experience in distributed system design\n2-7+ years of experience using Java, Python, and/or React\n2-7+ years of experience or interest in functional programming languages\nStrong object-oriented design and programming skills and experience in OO languages (Java)\nStrong experience with cloud infrastructure (AWS, Azure, or GCP) and infrastructure as code (Terraform, CloudFormation, or ARM templates).\nProven experience applying domain driven design to build complex business applications\nDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes\nIn-depth knowledge of relational and columnar SQL databases, including database design\nExpertise in data warehousing concepts (e.g. star schema, entitlement implementations, SQL v/s NoSQL modeling, milestoning, indexing, partitioning)\nExperience in REST and/or GraphQL\nExperience in creating Spark jobs for data transformation and aggregation\nComfort with Agile operating models (practical experience of Scrum / Kanban)\nGeneral knowledge of business processes, data flows and the quantitative models that generate or consume data\nExcellent communications skills and the ability to work with subject matter experts to extract critical business concepts\nIndependent thinker, willing to engage, challenge or learn\nAbility to stay commercially focused and to always push for quantifiable commercial impact\nStrong work ethic, a sense of ownership and urgency\nStrong analytical and problem solving skills\nEstablish trusted partnerships with key contacts and users across business and engineering teams\n\n\nPreferred Qualifications\n\n\nFinancial Services industry experience\nExperience with Pure/Legend\nWorking knowledge of open-source tools such as AWS lambda, Prometheus\n\n\nAbout Goldman Sachs\n\nAt Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.\n\nWe believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers.\n\nWe\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https://www.goldmansachs.com/careers/footer/disability-statement.html\nShow more "
  },
  {
    "job_id": "4221075747",
    "title": "Software Engineer I - Frontend",
    "company": "Nirvana Insurance",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/software-engineer-i-frontend-at-nirvana-insurance-4221075747?position=12&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=hHAAHwtsyYvMgCzwKwjXtg%3D%3D",
    "description": "Who We Are\n\nNirvana is on a mission to harness the power of data to revolutionize commercial insurance and enable a safer world. We are bringing much-needed innovation into the legacy, trillion-dollar commercial insurance industry. We have developed cutting-edge predictive models that use real-time IoT data from billions of connected devices, allowing us to better understand and price risk. Our AI-driven platform fundamentally changes the way an insurance company operates with personalized risk scoring, faster underwriting, modernized claims, and proactive, data-driven insights to help customers prevent accidents.\n\nWe\u2019ve already proven the scale\u2014reaching well over $100 million in premiums and more than doubling year over year. Our data moat is growing exponentially with more than 20 billion miles of telematics data, leading to more predictive models and new insights into how we can better understand and reduce risk. Altogether, our loss ratio, efficiency, and customer experience are redefining what can be done in the industry.\n\nWith $170+ million raised, including an industry-leading Series C round in January 2025, we\u2019re only accelerating our growth, with strong support from top-tier VCs including Lightspeed, General Catalyst, and Valor. Nirvana\u2019s leadership team has previously helped scale multi-billion-dollar companies from scratch, including Samsara, Rubrik, and Flexport, and includes industry veterans from Hiscox, The Hartford, and RLI.\n\nAbout The Role\n\nAs a Frontend Engineer at Nirvana, you\u2019ll play a key role in defining and building the user interface for our innovative insurance platform. Working closely with our product, design, and backend teams, you'll create engaging, efficient, and user-friendly web applications that help customers manage their insurance and safety operations effortlessly.\n\nYou will have a significant impact on the user experience and will contribute to shaping the frontend architecture, from the core framework to best practices, setting standards for the future of the platform.\n\nHere are some of the key areas you will work on as we take the product to market:\n\n\nUser-Centric Design: Develop responsive, intuitive, and accessible user interfaces that offer a seamless experience for customers engaging with our insurance and safety features.\nData Visualization: Design and implement dashboards that provide real-time insights from IoT data, including driving quality, vehicle health, and geospatial data.\nComponent Libraries: Build reusable, scalable UI components that streamline development while maintaining design consistency.\nCollaborative Workflows: Collaborate closely with backend engineers and designers to create user experiences that are functional, delightful, and effective.\n\n\nAbout You\n\nYou are passionate about delivering high-quality user experiences, crafting clean and maintainable code, and collaborating with diverse teams to achieve outstanding results. You thrive in environments where you can take ownership of the frontend stack and influence the product's direction.\n\n\nYou are a problem solver. You love breaking down complex user requirements and translating them into clean, efficient, and functional UIs.\nYou ship fast. You focus on shipping meaningful features rapidly while maintaining a high degree of quality and user satisfaction.\nYou have an eye for design. You care deeply about crafting elegant user interfaces and are passionate about UI/UX best practices.\nYou communicate clearly. You work well with both engineers and non-technical stakeholders, providing thoughtful code reviews and offering honest, constructive feedback.\nYou are adaptable. You embrace working on different parts of the UI stack and love tackling new challenges, even those outside your comfort zone.\n\n\nRequirements\n\n\n1+ years of experience building responsive, scalable web applications using modern JavaScript frameworks like React, Angular, or Vue.\nStrong proficiency with HTML, CSS, and JavaScript, with a deep understanding of building cross-browser, accessible web interfaces.\nExperience with Typescript will be preferable.\nExperience integrating frontend applications with backend services, including RESTful APIs.\nProficiency in building reusable component libraries and familiarity with design systems.\nKnowledge of modern React libraries like Tanstack, Material-ui, TailwindCSS, Vite, etc.\nExperience working in an agile environment, collaborating with designers and engineers.\n\n\nWhat You\u2019ll Get From Us\n\n\nCompetitive compensation & meaningful equity\nMedical insurance\nMonthly Wellness stipend for anything that gives you joy\nHybrid culture and reimbursement for home office equipment\nA flexible vacation policy and a team that understands building a company is a marathon, not a sprint\nA culture that gives you the autonomy you need to do great work, and the transparency you need to make good decisions\n\n\nShow more "
  },
  {
    "job_id": "4275881967",
    "title": "Software Engineer",
    "company": "NetApp",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/software-engineer-at-netapp-4275881967?position=13&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=URLQF550QBLGyNxV%2FsLhPA%3D%3D",
    "description": "Job Summary\n\nAs Software Engineer, you will be responsible for coming up with a test design / strategy for qualifying a feature and to develop, automate and execute tests using the NetApp\u2019s automation framework. This requirement is for the System Test Engineering group that is a centralized organization to test / qualify various features of ONTAP product line.\n\nJob Requirements\n\n\nRole is of \u201cSoftware Development in Testing\u201d (SDIT) that demands for a strong automation skill with a hands-on experience to be able to test & qualify a system under test.\nContribute to test methodologies, plans, and automation.\nA person should have a good understanding of test automation framework, scripting languages that enables the person to carry out automated tests who can triage & report issues.\nDesign, deploy, and maintain automated system tests to replicate real-world scenarios.\nUnderstand Functional specification and system architecture to properly design feature test plans.\nReview bug descriptions, functional requirements and design documents and incorporate them into test plans and cases.\nPossess strong communication skills who can articulate well to be able to work with an extended team consisting of developers and partner teams.\nWork with the support team to troubleshoot and reproduce customer impacting issues.\nProactive, positive attitude, a good team player, self-motivated and flexible.\nA person should be willing to work with partners across geographical locations who is expected to collaborate to deliver on commitments.\n\n\nTechnical Skills\n\n\nStrong Automation skillset who has hands on experience with Python is a must.\nBasic knowledge about storage/ storage protocols /cloud domain / K8s / Data protection techniques is preferable.\nBasic knowledge of Networking Protocols \u2013 TCP / IP, UDP / IP, VLAN etc. is preferable.\nExperience in QA / Testing\nFamiliarity with automation / testing frameworks\nSound knowledge in test design and test strategies\n\n\nEducation\n\nA Bachelor of Science Degree in Engineering or Computer Science with 2 years of experience, or a Master\u2019s Degree; or equivalent experience is required.\n\nAt NetApp, we embrace a hybrid working environment designed to strengthen connection, collaboration, and culture for all employees. This means that most roles will have some level of in-office and/or in-person expectations, which will be shared during the recruitment process.\n\nEqual Opportunity Employer\n\nNetApp is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all laws that prohibit employment discrimination based on age, race, color, gender, sexual orientation, gender identity, national origin, religion, disability or genetic information, pregnancy, and any protected classification.\n\nWhy NetApp?\n\nWe are all about helping customers turn challenges into business opportunity. It starts with bringing new thinking to age-old problems, like how to use data most effectively to run better - but also to innovate. We tailor our approach to the customer's unique needs with a combination of fresh thinking and proven approaches.\n\nWe enable a healthy work-life balance. Our volunteer time off program is best in class, offering employees 40 hours of paid time off each year to volunteer with their favourite organizations. We provide comprehensive benefits, including health care, life and accident plans, emotional support resources for you and your family, legal services, and financial savings programs to help you plan for your future. We support professional and personal growth through educational assistance and provide access to various discounts and perks to enhance your overall quality of life.\n\nIf you want to help us build knowledge and solve big problems, let's talk.\nShow more "
  },
  {
    "job_id": "4249842843",
    "title": "Staff Engineer, Software Development Engineering (Apps)- C++, STL/Boost programming",
    "company": "Sandisk",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/staff-engineer-software-development-engineering-apps-c%2B%2B-stl-boost-programming-at-sandisk-4249842843?position=14&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=Q5jZzvkCn2vMjgTbla20Vw%3D%3D",
    "description": "Company Description\n\nSandisk understands how people and businesses consume data and we relentlessly innovate to deliver solutions that enable today\u2019s needs and tomorrow\u2019s next big ideas. With a rich history of groundbreaking innovations in Flash and advanced memory technologies, our solutions have become the beating heart of the digital world we\u2019re living in and that we have the power to shape.\n\nSandisk meets people and businesses at the intersection of their aspirations and the moment, enabling them to keep moving and pushing possibility forward. We do this through the balance of our powerhouse manufacturing capabilities and our industry-leading portfolio of products that are recognized globally for innovation, performance and quality.\n\nSandisk has two facilities recognized by the World Economic Forum as part of the Global Lighthouse Network for advanced 4IR innovations. These facilities were also recognized as Sustainability Lighthouses for breakthroughs in efficient operations. With our global reach, we ensure the global supply chain has access to the Flash memory it needs to keep our world moving forward.\n\nJob Description\n\nEssential Duties and Responsibilities:\n\nMembers of the Software Tools Team work on design and implementation of tools for SSD products. Members of this team develop the in-house software tools used to interact with the SSDs and test their performance. Individual is expected to understand technical specifications (like protocol specs), design and implement features in the tools.\n\nQualifications\n\nMinimum Qualifications:\n\nRequired\n\n\nBE / B Tech / ME / M Tech / MS in Computer Science, Software Engineering, or Computer Engineering\n\n\nPreferred\n\n\nPrior experience with Windows driver development is nice to have.\nKnowledge of Python Language or any other scripting language is nice to have\nPrior experience working with storage protocols (SCSI, ATA, and NVMe).\n\n\nRequired Skills/Experience\n\n6-8 Years of relevant experience\n\n\nHands-on in C++, STL/Boost programming.\nStrong in data structures, design patterns.\nHands-on in multi-threaded design and programming.\nAbility to troubleshoot and debug complex issues.\nAble to work independently and perform in fast paced environment.\nPrior experience in working with Agile/Scrum.\n\n\nAdditional Information\n\nSandisk thrives on the power and potential of diversity. As a global company, we believe the most effective way to embrace the diversity of our customers and communities is to mirror it from within. We believe the fusion of various perspectives results in the best outcomes for our employees, our company, our customers, and the world around us. We are committed to an inclusive environment where every individual can thrive through a sense of belonging, respect and contribution.\n\nSandisk is committed to offering opportunities to applicants with disabilities and ensuring all candidates can successfully navigate our careers website and our hiring process. Please contact us at jobs.accommodations@sandisk.com to advise us of your accommodation request. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.\nShow more "
  },
  {
    "job_id": "4106054438",
    "title": "Regulatory Data Engineer",
    "company": "Sphera",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/regulatory-data-engineer-at-sphera-4106054438?position=15&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=arEKV1%2FQqvdQEVPye4UfGw%3D%3D",
    "description": "Sphera is a leading global provider of enterprise software and services that enables companies to manage and optimize their environmental, health, safety and sustainability. Our mission is to create a safer, more sustainable and productive world.\n\nSphera is a portfolio company of Blackstone, a U.S.-based alternative asset investment company that focuses on private equity, technology and innovation, and more. Blackstone businesses succeed through strong partnerships, a personalized approach and a commitment to exceptional performance with uncompromising integrity. Sphera and Blackstone are leaders in the Environmental, Social and Governance (ESG) space.\n\nWe are guided by our core values of Customer Centricity, Accountability, Bias to Action, Innovation, and Collaboration. These values help us recruit the right talent to join our rapidly expanding team of around the globe. It is important to us that each and every Spherion is not only eager to challenge themselves and knows how to get work done but is an awesome addition to our company culture.\n\nThis position focuses on collecting data on chemical substances from published regulatory or industry association sources. Examples include international inventory lists, occupational exposure limits, GHS classification lists, environmental emission restrictions (air, water, waste), transport of dangerous goods, and physical properties among other types of data.\n\nRoles And Responsibilities\n\n\nProactively monitor legislation maintained by Sphera by reviewing regulatory standards and interpretations, registers, gazettes, newsletters, etc. as related to Regulatory Data Content.\nIdentify applicable new and updated/revised legislation and obtain official source documents.\nAnalyze regulations for inclusion in Regulatory Content database.\nDevelop and maintain technical documentation (in pre-defined templates) that clearly describes the analysis and interpretation of the Regulatory Data Lists.\nPerform assigned data maintenance projects for a repository of over 230,000 regulated chemical substances. Be fully accountable for the individual update assignments in terms of quality and schedule.\nDevelop and maintain effective relationships with regulatory agency contacts and consultants specializing in specific regulations.\nLiaise with Content Development, Rules, QA, product management and other teams to identify and implement continuous improvement initiatives for content quality and work process efficiency enhancements.\nAttend weekly meetings with the Regulatory Data Analysts/Data team.\n\n\nRequired Education\n\n\nUniversity degree in Chemistry, Toxicology, Ecology or related fields. University or Advanced degree or professional certification in a related field is considered an asset.\n\n\nPreferred Experience\n\n\n0-2 years of experience working with EHS regulations or practical experience working with chemicals.\nExperience working with relational databases using programs such as MS Access or SQL.\nExperience with data manipulation in MS Excel.\nExperience with other MS office programs.\nGood knowledge of chemical nomenclature and chemical structures.\nFluent in English (both spoken and written) and strong reading/writing skills.\nPreferable knowledge of any other languages (i.e. French, German, Spanish or a language from Eastern Asia).\n\n\nOther Skills\n\n\nKnowledge of global chemical regulations and GHS; experience authoring Safety Data Sheets will be an advantage.\nAbility to assign chemical substances to applicable chemical groups based on properties such as solubility, metal content, petroleum streams, etc. will be an advantage.\nHigh level of attention to detail.\nHigh level of initiative.\nFlexible, adaptable to change.\nExcellent organizational, time-management and prioritization skills.\nExcellent interpersonal and communication skills.\n\n\nPhysical Requirements: Ability to physically perform general office requirements. Must be able to perform essential responsibilities with or without reasonable accommodations.\n\nSphera is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\n\nThis job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. This job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.\nShow more "
  },
  {
    "job_id": "4270387667",
    "title": "Data Analyst",
    "company": "myGwork - LGBTQ+ Business Community",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/data-analyst-at-mygwork-lgbtq%2B-business-community-4270387667?position=16&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=9oSLrMWcDwjz5eb5rvcIQw%3D%3D",
    "description": "This job is with Moody's, an inclusive employer and a member of myGwork \u2013 the largest global platform for the LGBTQ+ business community. Please do not contact the recruiter directly.\n\nAt Moody's, we unite the brightest minds to turn today\u2019s risks into tomorrow\u2019s opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are\u2014with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.\n\nIf you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.\n\nJob Description\n\nJob Title - Data Analyst\n\nEntity - Moody\u2019s Analytics - India\n\nLine of Business/Department - Customer, Operations & Risk (COR) \u2013 Business Systems\n\nLocation - India - Remote\n\nFull Time\n\nReporting to \u2013 Director of Data Analytics\n\nSkills and Competencies\n\n\nDemonstrated proficiency with SQL scripting/debugging, integration tools, and cloud warehouses - Required\nAbility to communicate complex concepts clearly and collaborate effectively with stakeholders - Required\nAbility to condense information into clear/tangible insights and story-tell with data - Required\nProven ability to work independently and as part of a distributed team, taking direction from multiple team members - Required\nKnowledgeable in data modelling techniques - Required\nExperience in reporting-based software, such as Tableau, PowerBI, and Qlik - Preferred\n\n\nEducation\n\n\nBS/BA/Advanced degree in Engineering, Computer Science, Business Analytics, Data Science or a related field\n\n\nResponsibilities\n\nThis role will be a key contributor to our technology, infrastructure, and operations from automating manual operational tasks, creating robust data transformation workflows, to supporting self-service analysis.\n\n\nModel raw data into clean, tested, and reusable datasets.\nTransform data by removing, flagging, or filtering inaccurate, sensitive, or corrupted records; aggregating data items; and joining database tables.\nEstablish data quality rules, requirements, and metrics to ensure accuracy and consistency.\nBuild and maintain dashboards and visualizations, including:\nIndividual analytics reports\nOperational dashboards\nStrategic dashboards (e.g., tracking KPIs or OKRs)\nAnalytical dashboards (e.g., identifying trends)\nForecasting tools\nDrive best practices for data handling, quality control, and change management across production reports and dashboards.\nDocument data practices to promote a shared language and consistent definitions among analysts.\nCreate reusable data assets that are ready for analysis.\nPartner with internal teams (e.g., Financial Planning & Analytics, Customer Success, Sales Performance & Analytics) to ensure high-quality user experiences and adoption of new reporting methods.\nLead automation and modernization initiatives for reporting structures and data visualizations.\nIdentify and implement process and workflow improvements using Generative/Agentic AI and other approved technologies.\nUnderstand business requirements and deliver clear, actionable insights that support customer-centric decision-making.\nCollaborate with internal stakeholders to standardize and enhance existing processes, improving operational efficiency and performance.\n\n\nAbout The Team\n\nThe Customer, Operations & Risk (COR) team at Moody\u2019s is a dynamic, cross-functional group that supports enterprise-wide systems, customer engagement, and risk management. COR encompasses several specialized functions including Customer Engagement, Business Systems, Business Integration & Operations, Risk, Sales Performance Management, and Learning & Development. The team plays a critical role in integrating acquisitions, modernizing operational platforms, and ensuring data quality and regulatory compliance across the organization. COR partners closely with business units to streamline the lead-to-cash lifecycle, enhance customer experience, and mitigate operational and data risks. This team is ideal for professionals who thrive in a collaborative, fast-paced environment and are passionate about driving operational excellence and strategic impact.\n\nMoody\u2019s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation, gender expression, gender identity or any other characteristic protected by law.\n\nCandidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody\u2019s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.\nShow more "
  },
  {
    "job_id": "4247916630",
    "title": "Software Engineer - Growth Engineering",
    "company": "Cloudflare",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/software-engineer-growth-engineering-at-cloudflare-4247916630?position=18&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=d%2B3ywUwEWUkmPteGo7RUxw%3D%3D",
    "description": "About Us\n\nAt Cloudflare, we are on a mission to help build a better Internet. Today the company runs one of the world\u2019s largest networks that powers millions of websites and other Internet properties for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine\u2019s Top Company Cultures list and ranked among the World\u2019s Most Innovative Companies by Fast Company.\n\nWe realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!\n\nAvailable Locations: Bengaluru\n\nAbout The Department\n\nThe Growth Engineering team is responsible for building world-class experiences that help the millions of Cloudflare self-service customers get what they need faster, from acquisition and onboarding all the way through to adoption and scale up. Our team is focused on high velocity experimentation and thoughtful optimizations to that experience on Cloudflare\u2019s properties.\n\nThis team has a dual mandate, also focusing on evolving our current marketing attribution, customer event ingress and experimentation capabilities that process billions of events across those properties to drive data-driven decision making.\n\nAs an engineer for the team responsible for Data Capture and Experimentation, your job will be to deliver on those growth-driven features and experiences while evolving our current marketing attribution, consumer event ingress and experimentation setup across these experiences, and partner with many teams on implementations.\n\nAbout The Role\n\nWe are looking for experienced full-stack engineers to join the Experimentation and Data Capture team. The ideal candidate will have experience working with large-scale applications, familiarity with event-driven data capture, and strong understanding of system design. You must care deeply not only about the quality of your and the team's code, but also the customer experience and developer experience. We have a great opportunity to evolve our current data capture and experimentation systems to better serve our customers.\n\nWe are also strong believers in dog-fooding our own products. From cache configuration to Cloudflare Access, Cloudflare Workers, and Zaraz, these are all tools in our engineer's tool belt, so it is a plus if you have been a customer of ours, even as a free user.\n\nWhat You\u2019ll Do\n\nThe Experimentation and Data Capture Engineering Team will be responsible for the following:\n\n\nTechnical delivery for Experimentation and Data Capture capabilities intended for all of our customer-facing UI properties, driving user acquisition, engagement, and retention through data-driven strategies and technical implementations\nCollaborate with product, design and stakeholders to establish outcome measurements, roadmaps and key deliverables\nOwn and lead execution of engineering projects in the area of web data acquisition and experimentation\nWork across the entire product lifecycle from conceptualization through production\nBuild features end-to-end: front-end, back-end, IaC, system design, debugging and testing, engaging with feature teams and data processing teams\nInspire and mentor less experienced engineers\nWork closely with the trust and safety team to handle any compliance or data privacy-related matters\n\n\n\nExamples Of Desirable Skills, Knowledge And Experience\n\n\nComfort with building reusable SDKs and UI components with TypeScript/JavaScript required, comfort/familiarity with other languages (Go/Rust/Python) a plus.\nExperience building with high-scale serverless systems like Cloudflare Workers, AWS Lambda, Azure Functions, etc.\nDesign and execute A/B tests and experiments to optimize for business KPIs, including user onboarding, feature adoption, and overall product experience. Create reusable components for other developers to leverage.\nExperience with publishing-to and querying-from data lake/warehouse products like Clickhouse, Apache Iceberg, to evaluate experiments. Familiarity with commercial analytics systems (Adobe Analytics, Google BigQuery, etc) a plus.\nImplement tracking and attribution systems to understand user behavior and measure the effectiveness of growth initiatives.\nFamiliarity with event driven architectures, high-scale data processing, issues that can occur and how to protect against them.\nFamiliarity with global data privacy requirements governed by laws like GDPR/CCPA/etc, and the implications for data capture, modeling, and analysis.\nDesire to work in a very fast-paced environment.\n\n\n\nWhat Makes Cloudflare Special?\n\nWe\u2019re not just a highly ambitious, large-scale technology company. We\u2019re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.\n\nProject Galileo: Since 2014, we've equipped more than 2,400 journalism and civil society organizations in 111 countries with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare\u2019s enterprise customers--at no cost.\n\nAthenian Project: In 2017, we created the Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration. Since the project, we've provided services to more than 425 local government election websites in 33 states.\n\n1.1.1.1: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here\u2019s the deal - we don\u2019t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.\n\nSound like something you\u2019d like to be a part of? We\u2019d love to hear from you!\n\nThis position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.\n\nCloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.\n\nCloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.\nShow more "
  },
  {
    "job_id": "4250267754",
    "title": "Machine Learning Engineer",
    "company": "AB InBev GCC India",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/machine-learning-engineer-at-ab-inbev-gcc-india-4250267754?position=19&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=pJSESzoNBzn%2FNKJA6swgZg%3D%3D",
    "description": "Dreaming big is in our DNA. It\u2019s who we are as a company. It\u2019s our culture. It\u2019s our heritage. And more than ever, it\u2019s our future. A future where we\u2019re always looking forward. Always serving up new ways to meet life\u2019s moments. A future where we keep dreaming bigger. We look for people with passion, talent, and curiosity, and provide them with the teammates, resources and opportunities to unleash their full potential. The power we create together \u2013 when we combine your strengths with ours \u2013 is unstoppable. Are you ready to join a team that dreams as big as you do?\n\nAB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\n\nDo You Dream Big?\n\nWe Need You.\n\nJob Description\n\nJob Title: Junior Data Scientist\n\nLocation: Bangalore\n\nReporting to: Senior Manager \u2013 Analytics\n\n\nPurpose of the role\n\nThe Global GenAI Team at Anheuser-Busch InBev (AB InBev) is tasked with constructing competitive solutions utilizing GenAI techniques. These solutions aim to extract contextual insights and meaningful information from our enterprise data assets. The derived data-driven insights play a pivotal role in empowering our business users to make well-informed decisions regarding their respective products. In the role of a Machine Learning Engineer (MLE), you will operate at the intersection of:\n\n\nLLM-based frameworks, tools, and technologies\nCloud-native technologies and solutions\nMicroservices-based software architecture and design patterns As an additional responsibility, you will be involved in the complete development cycle of new product features, encompassing tasks such as the development and deployment of new models integrated into production systems. Furthermore, you will have the opportunity to critically assess and influence the product engineering, design, architecture, and technology stack across multiple products, extending beyond your immediate focus.\nKey tasks & accountabilities\n\nLarge Language Models (LLM):\n\n\nExperience with LangChain, LangGraph\nProficiency in building agentic patterns like ReAct, ReWoo, LLMCompiler\n\nMulti-modal Retrieval-Augmented Generation (RAG):\n\n\nExpertise in multi-modal AI systems (text, images, audio, video)\nDesigning and optimizing chunking strategies and clustering for large data processing\n\nStreaming & Real-time Processing:\n\n\nExperience in audio/video streaming and real-time data pipelines\nLow-latency inference and deployment architectures\n\nNL2SQL:\n\n\nNatural language-driven SQL generation for databases\nExperience with natural language interfaces to databases and query optimization\n\nAPI Development:\n\n\nBuilding scalable APIs with FastAPI for AI model serving\n\nContainerization & Orchestration:\n\n\nProficient with Docker for containerized AI services\nExperience with orchestration tools for deploying and managing services\n\nData Processing & Pipelines:\n\n\nExperience with chunking strategies for efficient document processing\nBuilding data pipelines to handle large-scale data for AI model training and inference\n\nAI Frameworks & Tools:\n\n\nExperience with AI/ML frameworks like TensorFlow, PyTorch\nProficiency in LangChain, LangGraph, and other LLM-related technologies\n\nPrompt Engineering:\n\n\nExpertise in advanced prompting techniques like Chain of Thought (CoT) prompting, LLM Judge, and self-reflection prompting\nExperience with prompt compression and optimization using tools like LLMLingua, AdaFlow, TextGrad, and DSPy\nStrong understanding of context window management and optimizing prompts for performance and efficiency\n3. Qualifications, Experience, Skills\n\n\nLevel of educational attainment required (1 or more of the following)\n\n\nBachelor's or master\u02bcs degree in Computer Science, Engineering, or a related field.\n\nPrevious Work Experience Required\n\n\nProven experience of 3+ years in developing and deploying applications utilizing Azure OpenAI and Redis as a vector database.\n\nTechnical Skills Required\n\n\nSolid understanding of language model technologies, including LangChain, OpenAI Python SDK, LammaIndex, OLamma, etc.\nProficiency in implementing and optimizing machine learning models for natural language processing.\nExperience with observability tools such as mlflow, langsmith, langfuse, weight and bias, etc.\nStrong programming skills in languages such as Python and proficiency in relevant frameworks.\nFamiliarity with containerization and orchestration tools (e.g., Docker, Kubernetes).\n\nAnd above all of this, an undying love for beer!\n\nWe dream big to create future with more cheer\nShow more "
  },
  {
    "job_id": "4272455220",
    "title": "Azure Data Lake (ADLS) Developer/ Engineer",
    "company": "Infosys",
    "location": "Bengaluru East, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/azure-data-lake-adls-developer-engineer-at-infosys-4272455220?position=20&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=8p1O7eeqrxmkIyZDW2qVSg%3D%3D",
    "description": "Primary skills: Technology->Cloud Platform->Azure Analytics Services->Azure Data Lake\n\n\nA day in the life of an Infoscion\n\n\nAs part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nShow more "
  },
  {
    "job_id": "4234403464",
    "title": "Site Reliability Engineer - Big Data (5 to 7 years)",
    "company": "PhonePe",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/site-reliability-engineer-big-data-5-to-7-years-at-phonepe-4234403464?position=21&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=FyexmhVtXWxUGiwxcvLL2Q%3D%3D",
    "description": "About PhonePe Limited:\n\nHeadquartered in India, its flagship product, the PhonePe digital payments app, was launched in Aug 2016. As of April 2025, PhonePe has over 60 Crore (600 Million) registered users and a digital payments acceptance network spread across over 4 Crore (40+ million) merchants. PhonePe also processes over 33 Crore (330+ Million) transactions daily with an Annualized Total Payment Value (TPV) of over INR 150 lakh crore.\n\nPhonePe\u2019s portfolio of businesses includes the distribution of financial products (Insurance, Lending, and Wealth) as well as new consumer tech businesses (Pincode - hyperlocal e-commerce and Indus AppStore Localized App Store for the Android ecosystem) in India, which are aligned with the company\u2019s vision to offer every Indian an equal opportunity to accelerate their progress by unlocking the flow of money and access to services.\n\nCulture:\n\nAt PhonePe, we go the extra mile to make sure you can bring your best self to work, Everyday!. And that starts with creating the right environment for you. We empower people and trust them to do the right thing. Here, you own your work from start to finish, right from day one. PhonePe-rs solve complex problems and execute quickly; often building frameworks from scratch. If you\u2019re excited by the idea of building platforms that touch millions, ideating with some of the best minds in the country and executing on your dreams with purpose and speed, join us!\n\nAbout The Role\n\nAs an SRE (5 to 7 years) (Big Data) Engineer at PhonePe, you will be responsible for ensuring the stability, scalability, and performance of distributed systems operating at scale. You will collaborate with development, infrastructure, and data teams to automate operations, reduce manual efforts, handle incidents, and continuously improve system reliability. This role requires strong problem-solving skills, operational ownership, and a proactive approach to mentoring and driving engineering excellence.\n\nRoles And Responsibilities\n\n\nEnsure the ongoing stability, scalability, and performance of PhonePe\u2019s Hadoop ecosystem and associated services.\nManage and administer Hadoop infrastructure including HDFS, HBase, Hive, Pig, Airflow, YARN, Ranger, Kafka, Pinot, and Druid.\nAutomate BAU operations through scripting and tool development.\nPerform capacity planning, system tuning, and performance optimization.\nSet-up, configure, and manage Nginx in high-traffic environments.\nAdministration and troubleshooting of Linux + Bigdata systems, including networking (IP, Iptables, IPsec).\nHandle on-call responsibilities, investigate incidents, perform root cause analysis, and implement mitigation strategies.\nCollaborate with infrastructure, network, database, and BI teams to ensure data availability and quality.\nApply system updates, patches, and manage version upgrades in coordination with security teams.\nBuild tools and services to improve observability, debuggability, and supportability.\nParticipate in Kerberos and LDAP administration.\nExperience in capacity planning and performance tuning of Hadoop clusters.\nWork with configuration management and deployment tools like Puppet, Chef, Salt, or Ansible.\n\n\nSkills Required\n\n\nMinimum 1 year of Linux/Unix system administration experience.\nOver 4 years of hands-on experience in Hadoop administration.\nMinimum 1 years of experience managing infrastructure on public cloud platforms like AWS, Azure, or GCP (optional ) .\nStrong understanding of networking, open-source tools, and IT operations.\nProficient in scripting and programming (Perl, Golang, or Python).\nHands-on experience with maintaining and managing the Hadoop ecosystem components like HDFS, Yarn, Hbase, Kafka .\nStrong operational knowledge in systems (CPU, memory, storage, OS-level troubleshooting).\nExperience in administering and tuning relational and NoSQL databases.\nExperience in configuring and managing Nginx in production environments.\nExcellent communication and collaboration skills.\n\n\nGood to Have\n\n\nExperience designing and maintaining Airflow DAGs to automate scalable and efficient workflows.\nExperience in ELK stack administration.\nFamiliarity with monitoring tools like Grafana, Loki, Prometheus, and OpenTSDB.\nExposure to security protocols and tools (Kerberos, LDAP).\nFamiliarity with distributed systems like elasticsearch or similar high-scale environments.\n\n\nPhonePe Full Time Employee Benefits (Not applicable for Intern or Contract Roles)\n\n\nInsurance Benefits - Medical Insurance, Critical Illness Insurance, Accidental Insurance, Life Insurance\nWellness Program - Employee Assistance Program, Onsite Medical Center, Emergency Support System\nParental Support - Maternity Benefit, Paternity Benefit Program, Adoption Assistance Program, Day-care Support Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Higher Education Assistance, Car Lease, Salary Advance Policy\n\n\nOur inclusive culture promotes individual expression, creativity, innovation, and achievement and in turn helps us better understand and serve our customers. We see ourselves as a place for intellectual curiosity, ideas and debates, where diverse perspectives lead to deeper understanding and better quality results. PhonePe is an equal opportunity employer and is committed to treating all its employees and job applicants equally; regardless of gender, sexual preference, religion, race, color or disability. If you have a disability or special need that requires assistance or reasonable accommodation, during the application and hiring process, including support for the interview or onboarding process, please fill out this form.\n\nRead more about PhonePe on our blog.\n\nLife at PhonePe\n\nPhonePe in the news\nShow more "
  },
  {
    "job_id": "4232872582",
    "title": "Site Reliability Engineer - Big Data (7 to 11 years)",
    "company": "PhonePe",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/site-reliability-engineer-big-data-7-to-11-years-at-phonepe-4232872582?position=22&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=%2FSWa1nzB7taYm7X6b0Vb3w%3D%3D",
    "description": "About PhonePe Limited:\n\nHeadquartered in India, its flagship product, the PhonePe digital payments app, was launched in Aug 2016. As of April 2025, PhonePe has over 60 Crore (600 Million) registered users and a digital payments acceptance network spread across over 4 Crore (40+ million) merchants. PhonePe also processes over 33 Crore (330+ Million) transactions daily with an Annualized Total Payment Value (TPV) of over INR 150 lakh crore.\n\nPhonePe\u2019s portfolio of businesses includes the distribution of financial products (Insurance, Lending, and Wealth) as well as new consumer tech businesses (Pincode - hyperlocal e-commerce and Indus AppStore Localized App Store for the Android ecosystem) in India, which are aligned with the company\u2019s vision to offer every Indian an equal opportunity to accelerate their progress by unlocking the flow of money and access to services.\n\nCulture:\n\nAt PhonePe, we go the extra mile to make sure you can bring your best self to work, Everyday!. And that starts with creating the right environment for you. We empower people and trust them to do the right thing. Here, you own your work from start to finish, right from day one. PhonePe-rs solve complex problems and execute quickly; often building frameworks from scratch. If you\u2019re excited by the idea of building platforms that touch millions, ideating with some of the best minds in the country and executing on your dreams with purpose and speed, join us!\n\nAbout the Role:\n\nThis role is responsible for managing and maintaining complex, distributed big data ecosystems. It ensures the reliability, scalability, and security of large-scale production infrastructure. Key responsibilities include automating processes, optimizing workflows, troubleshooting production issues, and driving system improvements across multiple business verticals.\n\nRoles and Responsibilities:\n\n\nManage, maintain, and support incremental changes to Linux/Unix environments.\nLead on-call rotations and incident responses, conducting root cause analysis and driving postmortem processes.\nDesign and implement automation systems for managing big data infrastructure, including provisioning, scaling, upgrades, and patching clusters.\nTroubleshoot and resolve complex production issues while identifying root causes and implementing mitigating strategies.\nDesign and review scalable and reliable system architectures.\nCollaborate with teams to optimize overall system performance.\nEnforce security standards across systems and infrastructure.\nSet technical direction, drive standardization, and operate independently.\nEnsure availability, performance, and scalability of systems and services through proactive monitoring, maintenance, and capacity planning.\nResolve, analyze, and respond to system outages and disruptions and implement measures to prevent similar incidents from recurring.\nDevelop tools and scripts to automate operational processes, reducing manual workload, increasing efficiency and improving system resilience.\nMonitor and optimize system performance and resource usage, identify and address bottlenecks, and implement best practices for performance tuning.\nCollaborate with development teams to integrate best practices for reliability, scalability, and performance into the software development lifecycle.\nStay informed of industry technology trends and innovations, and actively contribute to the organization's technology communities.\nDevelop and enforce SRE best practices and principles.\nAlign across functional teams on priorities and deliverables.\nDrive automation to enhance operational efficiency.\n\n\nSkills Required:\n\n\nOver 7 years of experience managing and maintaining distributed big data ecosystems.\nStrong expertise in Linux including IP, Iptables, and IPsec.\nProficiency in scripting/programming with languages like Perl, Golang, or Python.\nHands-on experience with the Hadoop stack (HDFS, HBase, Airflow, YARN, Ranger, Kafka, Pinot).\nFamiliarity with open-source configuration management and deployment tools such as Puppet, Salt, Chef, or Ansible.\nSolid understanding of networking, open-source technologies, and related tools.\nExcellent communication and collaboration skills.\nDevOps tools: Saltstack, Ansible, docker, Git.\nSRE Logging and monitoring tools: ELK stack, Grafana, Prometheus, opentsdb, Open Telemetry.\n\n\nGood to Have:\n\n\nExperience managing infrastructure on public cloud platforms (AWS, Azure, GCP).\nExperience in designing and reviewing system architectures for scalability and reliability.\nExperience with observability tools to visualize and alert on system performance.\n\n\nPhonePe Full Time Employee Benefits (Not applicable for Intern or Contract Roles)\n\n\nInsurance Benefits - Medical Insurance, Critical Illness Insurance, Accidental Insurance, Life Insurance\nWellness Program - Employee Assistance Program, Onsite Medical Center, Emergency Support System\nParental Support - Maternity Benefit, Paternity Benefit Program, Adoption Assistance Program, Day-care Support Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Higher Education Assistance, Car Lease, Salary Advance Policy\n\n\nOur inclusive culture promotes individual expression, creativity, innovation, and achievement and in turn helps us better understand and serve our customers. We see ourselves as a place for intellectual curiosity, ideas and debates, where diverse perspectives lead to deeper understanding and better quality results. PhonePe is an equal opportunity employer and is committed to treating all its employees and job applicants equally; regardless of gender, sexual preference, religion, race, color or disability. If you have a disability or special need that requires assistance or reasonable accommodation, during the application and hiring process, including support for the interview or onboarding process, please fill out this form.\n\nRead more about PhonePe on our blog.\n\nLife at PhonePe\n\nPhonePe in the news\nShow more "
  },
  {
    "job_id": "4235365212",
    "title": "Embedded/Firmware Software Engineer",
    "company": "Arm",
    "location": "Bengaluru, Karnataka, India",
    "date": "2025-07-26",
    "link": "https://in.linkedin.com/jobs/view/embedded-firmware-software-engineer-at-arm-4235365212?position=24&pageNum=0&refId=VwpKnn0b3RigQ9E9sXYO4g%3D%3D&trackingId=67EtpD5iJ%2BOjf5yw2cO2cw%3D%3D",
    "description": "The Software (CE-SW) group is responsible for developing and improving the software ecosystem around Arm's next generation of applications processors and IP. This means working with processors and other hardware technology not yet available to the public. You will join a team of Software Engineers who share a passion for leaving their mark on the future of computing.\n\nJob Overview\n\nWe are looking for highly capable engineers to work in the areas of Client/Server/Automotive, ready to use their knowledge and experience to ensure we continue to deliver software with the level of quality demanded by our customers. If you are similarly passionate about innovative technologies, then we want to hear from you!\n\nResponsibilities\n\n\nActive involvement in the software design of reference application processor firmware.\nYour day to day role will involve low level software development, test and debug on various platforms, including software models, development boards and shipping products.\nCreating software stacks for Arm\u2019s reference platforms for future Arm devices.\nWe want you to be able to analyse industry specs, roadmap requirements, breakdown tasks and help implement the project plans.\nYour activities will involve upstreaming and maintenance.\n\n\nRequired Skills and Experience\n\n\n6+ Years work experience + University degree (or equivalent experience), ideally in a numerate subject.\nExcellent C/C++ programming skills with the ability to add significant new functionality, analyse and fix complex defects. Some knowledge of assembly and strong debugging skills are preferred.\nExpertise in application and low-level systems, with a strong understanding of system architecture (preferably ARM), OS fundamentals, bootloaders, and device drivers. Proficiency in Linux/Windows operating systems and driver development is preferred.\nStrong interpersonal skills; excellent written and spoken English.\n\n\n\u201cNice To Have\u201d Skills and Experience\n\n\nFamiliarity with open-source project development cycles and contribution processes.\nExperience of software profiling, instrumentation, and optimization.\nVerification and validation of firmware on both pre-silicon and post-silicon platforms.\nA knowledge of how to test software using various techniques alongside an awareness of the value of CI and automated test systems.\nFamiliarity and flexibility in the use of various software development lifecycle methods including Agile.\n\n\nIn Return\n\nYou will have the opportunity to learn about the latest Arm architecture features, working closely with highly skilled engineering teams on ground-breaking technology. You will be empowered to continually identify and roll out improvements to our ways of working.\n\nAccommodations at Arm\n\nAt Arm, we want to build extraordinary teams. If you need an adjustment or an accommodation during the recruitment process, please email accommodations@arm.com . To note, by sending us the requested information, you consent to its use by Arm to arrange for appropriate accommodations. All accommodation or adjustment requests will be treated with confidentiality, and information concerning these requests will only be disclosed as necessary to provide the accommodation. Although this is not an exhaustive list, examples of support include breaks between interviews, having documents read aloud, or office accessibility. Please email us about anything we can do to accommodate you during the recruitment process.\n\nHybrid Working at Arm\n\nArm\u2019s approach to hybrid working is designed to create a working environment that supports both high performance and personal wellbeing. We believe in bringing people together face to face to enable us to work at pace, whilst recognizing the value of flexibility. Within that framework, we empower groups/teams to determine their own hybrid working patterns, depending on the work and the team\u2019s needs. Details of what this means for each role will be shared upon application. In some cases, the flexibility we can offer is limited by local legal, regulatory, tax, or other considerations, and where this is the case, we will collaborate with you to find the best solution. Please talk to us to find out more about what this could look like for you.\n\nEqual Opportunities at Arm\n\nArm is an equal opportunity employer, committed to providing an environment of mutual respect where equal opportunities are available to all applicants and colleagues. We are a diverse organization of dedicated and innovative individuals, and don\u2019t discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\n\n\nShow more "
  }
]